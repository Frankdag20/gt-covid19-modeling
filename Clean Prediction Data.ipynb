{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District \", \"of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': 'US',\n",
       " '01': 'Alabama',\n",
       " '02': 'Alaska',\n",
       " '04': 'Arizona',\n",
       " '05': 'Arkansas',\n",
       " '06': 'California',\n",
       " '08': 'Colorado',\n",
       " '09': 'Connecticut',\n",
       " '10': 'Delaware',\n",
       " '11': 'District of Columbia',\n",
       " '12': 'Florida',\n",
       " '13': 'Georgia',\n",
       " '15': 'Hawaii',\n",
       " '16': 'Idaho',\n",
       " '17': 'Illinois',\n",
       " '18': 'Indiana',\n",
       " '19': 'Iowa',\n",
       " '20': 'Kansas',\n",
       " '21': 'Kentucky',\n",
       " '22': 'Louisiana',\n",
       " '23': 'Maine',\n",
       " '24': 'Maryland',\n",
       " '25': 'Massachusetts',\n",
       " '26': 'Michigan',\n",
       " '27': 'Minnesota',\n",
       " '28': 'Mississippi',\n",
       " '29': 'Missouri',\n",
       " '30': 'Montana',\n",
       " '31': 'Nebraska',\n",
       " '32': 'Nevada',\n",
       " '33': 'New Hampshire',\n",
       " '34': 'New Jersey',\n",
       " '35': 'New Mexico',\n",
       " '36': 'New York',\n",
       " '37': 'North Carolina',\n",
       " '38': 'North Dakota',\n",
       " '39': 'Ohio',\n",
       " '40': 'Oklahoma',\n",
       " '41': 'Oregon',\n",
       " '42': 'Pennsylvania',\n",
       " '44': 'Rhode Island',\n",
       " '45': 'South Carolina',\n",
       " '46': 'South Dakota',\n",
       " '47': 'Tennessee',\n",
       " '48': 'Texas',\n",
       " '49': 'Utah',\n",
       " '50': 'Vermont',\n",
       " '51': 'Virginia',\n",
       " '53': 'Washington',\n",
       " '54': 'West Virginia',\n",
       " '55': 'Wisconsin',\n",
       " '56': 'Wyoming',\n",
       " '60': 'American Samoa',\n",
       " '66': 'Guam',\n",
       " '69': 'Northern Mariana Islands',\n",
       " '72': 'Puerto Rico',\n",
       " '74': 'U.S. Minor Outlying Islands',\n",
       " '78': 'Virgin Islands'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary for FIPS code\n",
    "url = 'https://raw.githubusercontent.com/reichlab/covid19-forecast-hub/master/data-locations/locations.csv'\n",
    "df_fips = pd.read_csv(url, error_bad_lines=False)\n",
    "us_state_fips = {}\n",
    "for index, row in df_fips.iterrows():\n",
    "    if index < 58 and index >= 0:\n",
    "        us_state_fips[row[\"location\"]] = row[\"location_name\"]\n",
    "us_state_fips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Getting predictions from CDC COVID forecast Github <h1>\n",
    "<h6> This will clean data as the format we want later for visualizations and measurements purposes. This will generate 3 csv files for each model: model_cum_death.csv, model_inc_death.csv, model_inc_case.csv. They will be stored under each model's own folder under \"./Predictions/\".<h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all filenames under a specific model from CDC COVID forecast Github\n",
    "def get_filenames(model):\n",
    "    #plese put your Github account username and password here to use Github API\n",
    "    g = Github(\"\",\"\")\n",
    "    repo = g.get_repo(\"reichlab/covid19-forecast-hub\")\n",
    "    contents = repo.get_contents(\"/data-processed/\" + model)\n",
    "    filenames = []\n",
    "    for content_file in contents:\n",
    "        filenames.append(content_file.name)\n",
    "    \n",
    "    #preserve only .csv files\n",
    "    to_remove = []\n",
    "    for filename in filenames:\n",
    "        if not filename[-3:] == 'csv':\n",
    "            to_remove.append(filename)\n",
    "    for item in to_remove:\n",
    "        filenames.remove(item)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all models names from CDC COVID forecast Github\n",
    "def get_modelnames():\n",
    "    #plese put your Github account username and password here to use Github API\n",
    "    g = Github(\"\",\"\")\n",
    "    repo = g.get_repo(\"reichlab/covid19-forecast-hub\")\n",
    "    contents = repo.get_contents(\"/data-processed/\")\n",
    "    filenames = []\n",
    "    for content_file in contents:\n",
    "        filenames.append(content_file.name)\n",
    "    \n",
    "    #preserve only .csv files\n",
    "    to_remove = []\n",
    "    for filename in filenames:\n",
    "        if '.' in filename:\n",
    "            to_remove.append(filename)\n",
    "    \n",
    "    for item in to_remove:\n",
    "        filenames.remove(item)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model):\n",
    "    filenames = get_filenames(model)\n",
    "    df_cum_death = pd.DataFrame(columns = [\"state\", \"# weeks ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_death = pd.DataFrame(columns = [\"state\", \"# weeks ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_case = pd.DataFrame(columns = [\"state\", \"# weeks ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            url = 'https://raw.githubusercontent.com/reichlab/covid19-forecast-hub/master/data-processed/' + model + '/' + filename\n",
    "            df = pd.read_csv(url, error_bad_lines=False, dtype={\"location\":str})\n",
    "            df.set_index([\"location\", \"type\"],inplace=True)\n",
    "\n",
    "            new_cum_death = {}\n",
    "            new_inc_death = {}\n",
    "            new_inc_case = {}\n",
    "\n",
    "            state_list_temp = list(dict.fromkeys(df.index.get_level_values(0)))\n",
    "            state_list = []\n",
    "            #remove county level \"fips\" code (zip code)\n",
    "            for state in state_list_temp:\n",
    "                if state in list(us_state_fips.keys()):\n",
    "                    state_list.append(state)\n",
    "\n",
    "            for state in state_list:\n",
    "                for index, row in df.loc[(state,\"point\")].iterrows():\n",
    "                    if row[\"target\"][-9:] == \"cum death\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_cum_death[week_ahead] = {}\n",
    "                        new_cum_death[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_cum_death[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_cum_death[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_cum_death[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_cum_death[week_ahead][\"# weeks ahead\"] = week_ahead\n",
    "                    if row[\"target\"][-9:] == \"inc death\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_inc_death[week_ahead] = {}\n",
    "                        new_inc_death[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_inc_death[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_inc_death[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_inc_death[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_inc_death[week_ahead][\"# weeks ahead\"] = week_ahead\n",
    "                    if row[\"target\"][-9:] == \" inc case\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_inc_case[week_ahead] = {}\n",
    "                        new_inc_case[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_inc_case[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_inc_case[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_inc_case[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_inc_case[week_ahead][\"# weeks ahead\"] = week_ahead\n",
    "\n",
    "                for key in new_cum_death:\n",
    "                    df_cum_death = df_cum_death.append(new_cum_death[key],ignore_index=True)\n",
    "                for key in new_inc_death:\n",
    "                    df_inc_death = df_inc_death.append(new_inc_death[key],ignore_index=True)\n",
    "                for key in new_inc_case:\n",
    "                    df_inc_case = df_inc_case.append(new_inc_case[key],ignore_index=True)\n",
    "        except:\n",
    "                print(filename + \" failed.\")\n",
    "                    \n",
    "    df_cum_death.set_index(['state',\"forecast_date\", \"# weeks ahead\"], inplace=True)\n",
    "    df_cum_death.sort_index(inplace=True)\n",
    "    df_inc_death.set_index(['state',\"forecast_date\", \"# weeks ahead\"], inplace=True)\n",
    "    df_inc_death.sort_index(inplace=True)\n",
    "    df_inc_case.set_index(['state',\"forecast_date\", \"# weeks ahead\"], inplace=True)\n",
    "    df_inc_case.sort_index(inplace=True)\n",
    "    \n",
    "    outdir = './Predictions/' + model\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    if not df_cum_death.empty:\n",
    "        df_cum_death.to_csv(os.path.join(outdir, model + \"_cum_death.csv\") )\n",
    "    if not df_inc_death.empty:\n",
    "        df_inc_death.to_csv(os.path.join(outdir, model + \"_inc_death.csv\") )\n",
    "    if not df_inc_case.empty:\n",
    "        df_inc_case.to_csv(os.path.join(outdir, model + \"_inc_case.csv\") )\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-4d718729c08d>:24: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  for index, row in df.loc[(state,\"point\")].iterrows():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24-Auquan-SEIR.csv failed.\n",
      "2020-06-07-Auquan-SEIR.csv failed.\n"
     ]
    }
   ],
   "source": [
    "#run this cell to get all data cleaned and store locally.\n",
    "models = get_modelnames()\n",
    "for model in models:\n",
    "    get_prediction(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Getting ground truth data from JHU COVID Github <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jhu():\n",
    "    #cum confirmed\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "    df_c = pd.read_csv(url, error_bad_lines=False)\n",
    "    df_cGrouped = df_c.groupby('Province_State')\n",
    "    df_c = df_cGrouped.sum()\n",
    "    column_names = list(df_c.columns)\n",
    "    to_delete = column_names[0:44]\n",
    "    df_c.drop(to_delete,axis=1,inplace=True)\n",
    "    \n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "    df_us = pd.read_csv(url, error_bad_lines=False)\n",
    "    new_row = pd.Series(df_us.iloc[-24,:])\n",
    "    to_remove = list(new_row.index[:43])\n",
    "    new_row.drop(labels=to_remove,inplace=True)\n",
    "    new_row.name=\"US\"\n",
    "    df_c = df_c.append(new_row,ignore_index=False)\n",
    "    \n",
    "    column_names = [datetime.strptime(d[:-3]+\"/2020\", '%m/%d/%Y').date().strftime(\"%Y-%m-%d\") for d in column_names[44:]]\n",
    "    prev_column_names = list(df_c.columns)\n",
    "    dic = {}\n",
    "    for i in range(len(column_names)):\n",
    "        dic[prev_column_names[i]] = column_names[i]\n",
    "    df_c.rename(columns=dic,inplace=True)\n",
    "\n",
    "    \n",
    "    #cum death\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "    df_d = pd.read_csv(url, error_bad_lines=False)\n",
    "    df_dGrouped = df_d.groupby('Province_State')\n",
    "    df_d = df_dGrouped.sum()\n",
    "    column_names = list(df_d.columns)\n",
    "    to_delete = column_names[0:45]\n",
    "    df_d.drop(to_delete,axis=1,inplace=True)\n",
    "    \n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
    "    df_us = pd.read_csv(url, error_bad_lines=False)\n",
    "    new_row = pd.Series(df_us.iloc[-24,:])\n",
    "    to_remove = list(new_row.index[:43])\n",
    "    new_row.drop(labels=to_remove,inplace=True)\n",
    "    new_row.name=\"US\"\n",
    "    df_d = df_d.append(new_row,ignore_index=False)\n",
    "    \n",
    "    column_names = [datetime.strptime(d[:-3]+\"/2020\", '%m/%d/%Y').date().strftime(\"%Y-%m-%d\") for d in column_names[45:]]\n",
    "    prev_column_names = list(df_d.columns)\n",
    "    dic = {}\n",
    "    for i in range(len(column_names)):\n",
    "        dic[prev_column_names[i]] = column_names[i]\n",
    "    df_d.rename(columns=dic,inplace=True)\n",
    "    \n",
    "    #calculation inc death and inc case\n",
    "    df_inc_d = df_d.copy()\n",
    "    df_inc_c = df_c.copy()\n",
    "    df_inc_d.drop([\"2020-03-01\"],axis=1,inplace=True)\n",
    "    df_inc_c.drop([\"2020-03-01\"],axis=1,inplace=True)\n",
    "    \n",
    "    for state in df_c.index:\n",
    "        for i in range(1,len(df_c.columns)):\n",
    "            df_inc_c.loc[state][df_c.columns[i]] = df_c.loc[state][df_c.columns[i]] - df_c.loc[state][df_c.columns[i-1]]\n",
    "            df_inc_d.loc[state][df_d.columns[i]] = df_d.loc[state][df_d.columns[i]] - df_d.loc[state][df_d.columns[i-1]]\n",
    "    \n",
    "    df_inc_d[df_inc_d<0]=0\n",
    "    df_inc_c[df_inc_c<0]=0\n",
    "    \n",
    "    df_d.to_csv(\"./Ground Truth/cum_death.csv\")\n",
    "    df_inc_d.to_csv(\"./Ground Truth/inc_death.csv\")\n",
    "    df_inc_c.to_csv(\"./Ground Truth/inc_case.csv\")\n",
    "    df_c.to_csv(\"./Ground Truth/cum_case.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to get ground truth data cleaned and store locally\n",
    "jhu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> This part is just our efforts to reformat our model prediction so that it can match cdc's format. Please ignore.<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir(\"./Predictions_OurModels/LSTM\")\n",
    "for file in filename:\n",
    "    if file[-9:] == 'daily.csv':\n",
    "        os.remove(\"./Predictions_OurModels/LSTM/\" + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "filename = os.listdir(\"./Predictions_OurModels/SIRD\")\n",
    "for file in filename:\n",
    "    if file[-11:] == 'rt-live.csv':\n",
    "        shutil.move(\"./Predictions_OurModels/SIRD/\" + file,'./Predictions_OurModels/SIRD-rt-live')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"LSTM\", \"SIR\", \"SIRD\", \"RandomForest\", \"SIRD-rt-live\", \"ARIMA\", \"ARGOnet\"]\n",
    "for name in l:\n",
    "    filename = os.listdir(\"./Predictions_OurModels/\" + name)\n",
    "    to_remove = []\n",
    "    for i in filename:\n",
    "        if not i[-3:] == 'csv':\n",
    "            to_remove.append(i)\n",
    "    for i in to_remove:\n",
    "        filename.remove(i)\n",
    "    \n",
    "    for file in filename:\n",
    "        df = pd.read_csv(\"./Predictions_OurModels/\" + name + \"/\" + file)\n",
    "        if name in [\"LSTM\", \"SIR\", \"SIRD\", \"RandomForest\", \"SIRD-rt-live\"]:\n",
    "            df[\"type\"] = \"point\"\n",
    "            df.rename(columns={\"point\":\"value\"},inplace=True)\n",
    "            df[\"quantile\"] = \"NA\"\n",
    "        for index, row in df.iterrows():\n",
    "            num = (datetime.strptime(row[\"target_end_date\"],\"%Y-%m-%d\").date() - datetime.strptime(row[\"forecast_date\"],\"%Y-%m-%d\").date()).days\n",
    "            if row[\"target\"][-9:] == \" cum case\" or row[\"target\"] == \"Cumulative Cases\":\n",
    "                df.at[index,\"target\"] = str(num) + \" day ahead cum case\"\n",
    "            if row[\"target\"][-9:] == \" inc case\" or row[\"target\"] == \"Daily Cases\":\n",
    "                df.at[index,\"target\"] = str(num) + \" day ahead inc case\"\n",
    "            if row[\"target\"][-9:] == \"cum death\":\n",
    "                df.at[index,\"target\"] = str(num) + \" day ahead cum death\"\n",
    "            if row[\"target\"][-9:] == \"inc death\":\n",
    "                df.at[index,\"target\"] = str(num) + \" day ahead inc death\"\n",
    "            \n",
    "        df.to_csv(\"./Predictions_OurModels/\" + name + \"/\" + file,index=False)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_GITIDEAS(model):\n",
    "    filenames = os.listdir(\"./Predictions_OurModels/\" + model)\n",
    "    df_cum_death = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_death = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_case = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_cum_case = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            url = './Predictions_OurModels/' + model + '/' + filename\n",
    "            df = pd.read_csv(url, error_bad_lines=False, dtype={\"location\":str})\n",
    "            df.set_index([\"location\", \"type\"],inplace=True)\n",
    "\n",
    "            new_cum_death = {}\n",
    "            new_inc_death = {}\n",
    "            new_inc_case = {}\n",
    "            new_cum_case = {}\n",
    "\n",
    "            state_list = list(dict.fromkeys(df.index.get_level_values(0)))\n",
    "            \n",
    "            for state in state_list:\n",
    "                for index, row in df.loc[(state,\"point\")].iterrows():\n",
    "                    if row[\"target\"][0] == \"0\":\n",
    "                        continue\n",
    "                    if row[\"target\"][-9:] == \"cum death\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_cum_death[week_ahead] = {}\n",
    "                        new_cum_death[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_cum_death[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_cum_death[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_cum_death[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_cum_death[week_ahead][\"# days ahead\"] = week_ahead\n",
    "                    if row[\"target\"][-9:] == \"inc death\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_inc_death[week_ahead] = {}\n",
    "                        new_inc_death[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_inc_death[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_inc_death[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_inc_death[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_inc_death[week_ahead][\"# days ahead\"] = week_ahead\n",
    "                    if row[\"target\"][-9:] == \" inc case\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_inc_case[week_ahead] = {}\n",
    "                        new_inc_case[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_inc_case[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_inc_case[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_inc_case[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_inc_case[week_ahead][\"# days ahead\"] = week_ahead\n",
    "                    if row[\"target\"][-9:] == \" cum case\":\n",
    "                        week_ahead = row[\"target\"][:2]\n",
    "                        new_cum_case[week_ahead] = {}\n",
    "                        new_cum_case[week_ahead][\"state\"] = us_state_fips[index[0]]\n",
    "                        new_cum_case[week_ahead][\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                        new_cum_case[week_ahead][\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                        new_cum_case[week_ahead][\"mean\"] = row[\"value\"]\n",
    "                        new_cum_case[week_ahead][\"# days ahead\"] = week_ahead\n",
    "\n",
    "                for key in new_cum_death:\n",
    "                    df_cum_death = df_cum_death.append(new_cum_death[key],ignore_index=True)\n",
    "                for key in new_inc_death:\n",
    "                    df_inc_death = df_inc_death.append(new_inc_death[key],ignore_index=True)\n",
    "                for key in new_inc_case:\n",
    "                    df_inc_case = df_inc_case.append(new_inc_case[key],ignore_index=True)\n",
    "                for key in new_cum_case:\n",
    "                    df_cum_case = df_cum_case.append(new_cum_case[key],ignore_index=True)\n",
    "        except:\n",
    "                print(filename + \" failed.\")\n",
    "                    \n",
    "    df_cum_death.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_cum_death.sort_index(inplace=True)\n",
    "    df_inc_death.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_inc_death.sort_index(inplace=True)\n",
    "    df_inc_case.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_inc_case.sort_index(inplace=True)\n",
    "    \n",
    "    outdir = './Predictions/' + model\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    if not df_cum_death.empty:\n",
    "        df_cum_death.to_csv(os.path.join(outdir, model + \"_cum_death.csv\") )\n",
    "    if not df_inc_death.empty:\n",
    "        df_inc_death.to_csv(os.path.join(outdir, model + \"_inc_death.csv\") )\n",
    "    if not df_inc_case.empty:\n",
    "        df_inc_case.to_csv(os.path.join(outdir, model + \"_inc_case.csv\") )\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store failed.\n",
      ".DS_Store failed.\n",
      ".ipynb_checkpoints failed.\n",
      ".DS_Store failed.\n",
      ".ipynb_checkpoints failed.\n",
      ".DS_Store failed.\n",
      ".ipynb_checkpoints failed.\n",
      ".DS_Store failed.\n"
     ]
    }
   ],
   "source": [
    "models = [\"LSTM\", \"SIR\", \"SIRD\", \"RandomForest\", \"SIRD-rt-live\"]\n",
    "for model in models:\n",
    "    get_prediction_GITIDEAS(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-2a5128536ef2>:16: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  for index, row in df.loc[(state,\"point\")].iterrows():\n"
     ]
    }
   ],
   "source": [
    "models = [\"ARIMA\",\"ARGOnet\"]\n",
    "for model in models:\n",
    "    filename = os.listdir(\"./Predictions_OurModels/\" + model)\n",
    "    filename.remove('.DS_Store')\n",
    "    df = pd.read_csv('./Predictions_OurModels/' + model + '/' + filename[0], error_bad_lines=False, dtype={\"location\":str})\n",
    "    df.set_index([\"location\", \"type\"],inplace=True)\n",
    "    \n",
    "    df_cum_death = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_death = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_inc_case = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "    df_cum_case = pd.DataFrame(columns = [\"state\", \"# days ahead\", \"forecast_date\", \"target_end_date\", \"mean\"])\n",
    "\n",
    "    state_list = list(dict.fromkeys(df.index.get_level_values(0)))\n",
    "    \n",
    "    for state in state_list:\n",
    "        for index, row in df.loc[(state,\"point\")].iterrows():\n",
    "            if row[\"target\"][-9:] == \"cum death\":\n",
    "                new_cum_death = {}\n",
    "                new_cum_death[\"state\"] = us_state_fips[index[0]]\n",
    "                new_cum_death[\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                new_cum_death[\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                new_cum_death[\"mean\"] = row[\"value\"]\n",
    "                new_cum_death[\"# days ahead\"] = row[\"target\"][:2]\n",
    "                df_cum_death = df_cum_death.append(new_cum_death,ignore_index=True)\n",
    "            if row[\"target\"][-9:] == \"inc death\":\n",
    "                new_inc_death = {}\n",
    "                new_inc_death[\"state\"] = us_state_fips[index[0]]\n",
    "                new_inc_death[\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                new_inc_death[\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                new_inc_death[\"mean\"] = row[\"value\"]\n",
    "                new_inc_death[\"# days ahead\"] = row[\"target\"][:2]\n",
    "                df_inc_death = df_inc_death.append(new_inc_death,ignore_index=True)\n",
    "            if row[\"target\"][-9:] == \" inc case\":\n",
    "                new_inc_case = {}\n",
    "                new_inc_case[\"state\"] = us_state_fips[index[0]]\n",
    "                new_inc_case[\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                new_inc_case[\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                new_inc_case[\"mean\"] = row[\"value\"]\n",
    "                new_inc_case[\"# days ahead\"] = row[\"target\"][:2]\n",
    "                df_inc_case = df_inc_case.append(new_inc_case,ignore_index=True)\n",
    "            if row[\"target\"][-9:] == \" cum case\":\n",
    "                new_cum_case = {}\n",
    "                new_cum_case[\"state\"] = us_state_fips[index[0]]\n",
    "                new_cum_case[\"target_end_date\"] = row[\"target_end_date\"]\n",
    "                new_cum_case[\"forecast_date\"] = row[\"forecast_date\"]\n",
    "                new_cum_case[\"mean\"] = row[\"value\"]\n",
    "                new_cum_case[\"# days ahead\"] = row[\"target\"][:2]\n",
    "                df_cum_case = df_cum_case.append(new_cum_case,ignore_index=True)\n",
    "\n",
    "    df_cum_death.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_cum_death.sort_index(inplace=True)\n",
    "    df_inc_death.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_inc_death.sort_index(inplace=True)\n",
    "    df_inc_case.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_inc_case.sort_index(inplace=True)\n",
    "    df_cum_case.set_index(['state',\"forecast_date\", \"# days ahead\"], inplace=True)\n",
    "    df_cum_case.sort_index(inplace=True)\n",
    "    \n",
    "    outdir = './Predictions/' + model\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    if not df_cum_death.empty:\n",
    "        df_cum_death.to_csv(os.path.join(outdir, model + \"_cum_death.csv\") )\n",
    "    if not df_inc_death.empty:\n",
    "        df_inc_death.to_csv(os.path.join(outdir, model + \"_inc_death.csv\") )\n",
    "    if not df_inc_case.empty:\n",
    "        df_inc_case.to_csv(os.path.join(outdir, model + \"_inc_case.csv\") )\n",
    "    if not df_cum_case.empty:\n",
    "        df_cum_case.to_csv(os.path.join(outdir, model + \"_cum_case.csv\") )\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
